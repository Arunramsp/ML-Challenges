{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bececbbba8deee8a46217e4d69fca445d0e188ab",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport random\nimport xgboost as xgb\n\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom sklearn.preprocessing import OneHotEncoder,LabelEncoder\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom math import sqrt\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge\nfrom sklearn.kernel_ridge import KernelRidge\n\ndef apply_log(train,test):\n    train[\"Upvotes\"] = np.log1p(train[\"Upvotes\"])\n    train[\"Reputation\"] = np.log1p(train[\"Reputation\"])\n    train[\"Views\"] = np.log1p(train[\"Views\"])\n    train[\"Answers\"] = np.log1p(train[\"Answers\"])\n\n    test[\"Reputation\"] = np.log1p(test[\"Reputation\"])\n    test[\"Views\"] = np.log1p(test[\"Views\"])\n    test[\"Answers\"] = np.log1p(test[\"Answers\"])\n    return train,test\n\ndef get_label_id(train,test):\n    #Drop the irrelevant features from the test set\n    ID_train = train['ID']\n    train_labels = np.array(train['Upvotes'])\n    train_features = train.drop(['Upvotes','ID','Username','Tag'], axis=1)\n    train_features = train_features.fillna(0)\n    #Drop the irrelevant features from the test set\n    ID_test = test['ID']\n    test_features = test.drop(['ID','Username','Tag'], axis=1)\n    test_features = test_features.fillna(0)\n    \n    return ID_train,train_features,train_labels,ID_test,test_features\n    \n\ndef train_mod(train_features,test_features,train_labels):\n    st_train = train_features.values\n    st_test = test_features.values\n    Y = train_labels\n#     clf = lgb.LGBMRegressor()\n#     clf = xgb.XGBRegressor()\n#     clf = LinearRegression()\n    clf = BayesianRidge()\n    # clf = lgb.LGBMRegressor(max_depth=6,learning_rate=0.0716,n_estimators=128,num_leaves=24,reg_alpha=1.7250,reg_lambda=0.0888,subsample=0.6361,colsample_bytree=0.9365)\n#     clf = xgb.XGBRegressor(gamma = 0.76,learning_rate = 0.0100,max_depth = 5,min_child_weight = 2,n_estimators = 107,subsample = 0.60,colsample_bytree = 0.9900)\n#     clf = xgb.XGBRegressor(gamma = 0.9257,learning_rate = 0.2797,max_depth = 5,min_child_weight = 9,n_estimators = 305,subsample = 0.6239,colsample_bytree = 0.7443)\n#     clf = xgb.XGBRegressor(gamma = 0.9019,learning_rate = 0.2530,max_depth = 4,min_child_weight = 8,n_estimators = 300,subsample = 0.6409,colsample_bytree = 0.7380)\n    fold = 5\n    cv = KFold(n_splits=fold, shuffle=True, random_state=42)\n    X_preds = np.zeros(st_train.shape[0])\n    preds = np.zeros(st_test.shape[0])\n    for i, (tr, ts) in enumerate(cv.split(st_train)):\n        print(ts.shape)\n        mod = clf.fit(st_train[tr], Y[tr])\n        X_preds[ts] = mod.predict(st_train[ts])\n        preds += mod.predict(st_test)\n        print(\"fold {}, RMSE : {:.3f}\".format(i, sqrt(mean_squared_error(Y[ts], X_preds[ts]))))\n    score = sqrt(mean_squared_error(Y, X_preds))\n    print(score)\n    preds1 = preds/fold\n    preds1 = np.abs(np.expm1(preds1))\n    X_preds = np.abs(np.expm1(X_preds))\n    return X_preds,preds1\n\ndef save_off(train_ID,X_preds,test_id,preds1):\n    tr_sub = pd.DataFrame({'ID': train_ID, 'Upvotes': X_preds})\n    tr_sub=tr_sub.reindex(columns=[\"ID\",\"Upvotes\"])\n    tr_sub.to_csv('train_oof.csv', index=False)\n\n    sub = pd.DataFrame({'ID': test_id, 'Upvotes': preds1})\n    sub=sub.reindex(columns=[\"ID\",\"Upvotes\"])\n    sub.to_csv('submission.csv', index=False)\n\ntrain = pd.read_csv('../input/train_NIR5Yl1.csv')\ntest = pd.read_csv('../input/test_8i3B3FC.csv')\n\ntest_id=[]\ntrain_id=[]\ntest_pred=[]\ntrain_pred=[]\n\nprint(train.shape)\nprint(test.shape)\nprint(train.columns)\nprint(train.Tag.unique())\n\n#train1 = train1.drop(train1[(train1['Views']>3100000) | (train1['Reputation'] > 900000) | (train1['Upvotes'] > 210000) | (train1['Answers'] > 65)].index)\n#array(['a', 'c', 'r', 'j', 'p', 's', 'h', 'o', 'i', 'x']\n\n##a##\ntrain1 = train[train.Tag == 'a']\ntest1 = test[test.Tag == 'a']\nprint(\"Before train1 drop: \",train1.shape)\nprint(\"Before test1 drop\",test1.shape)\ntrain1 = train1.drop(train1[(train1['Views']>2000000) | (train1['Upvotes'] > 100000) | (train1['Answers'] > 50)].index)\nprint(\"After train1 drop: \",train1.shape)\n\ntrain1,test1 = apply_log(train1,test1)\nID_train,train_features,train_labels,ID_test,test_features = get_label_id(train1,test1)\nX_preds,preds1 = train_mod(train_features,test_features,train_labels)\n\ntest_id.extend(ID_test)\ntrain_id.extend(ID_train)\ntest_pred.extend(preds1)\ntrain_pred.extend(X_preds)\n\n##c##\ntrain1 = train[train.Tag == 'c']\ntest1 = test[test.Tag == 'c']\ntrain1 = train1.drop(train1[(train1['Views']>1700000) | (train1['Upvotes'] > 150000) | (train1['Answers'] > 50)].index)\n\ntrain1,test1 = apply_log(train1,test1)\nID_train,train_features,train_labels,ID_test,test_features = get_label_id(train1,test1)\nX_preds,preds1 = train_mod(train_features,test_features,train_labels)\n\ntest_id.extend(list(ID_test))\ntrain_id.extend(list(ID_train))\ntest_pred.extend(list(preds1))\ntrain_pred.extend(list(X_preds))\n\n##r##\ntrain1 = train[train.Tag == 'r']\ntest1 = test[test.Tag == 'r']\ntrain1 = train1.drop(train1[(train1['Views']>600000) | (train1['Upvotes'] > 80000) | (train1['Answers'] > 20)].index)\n\ntrain1,test1 = apply_log(train1,test1)\nID_train,train_features,train_labels,ID_test,test_features = get_label_id(train1,test1)\nX_preds,preds1 = train_mod(train_features,test_features,train_labels)\n\ntest_id.extend(list(ID_test))\ntrain_id.extend(list(ID_train))\ntest_pred.extend(list(preds1))\ntrain_pred.extend(list(X_preds))\n\n##j##\ntrain1 = train[train.Tag == 'j']\ntest1 = test[test.Tag == 'j']\ntrain1 = train1.drop(train1[(train1['Views']>3000000) | (train1['Upvotes'] > 300000) | (train1['Answers'] > 60) | (train1['Reputation'] > 700000)].index)\n\ntrain1,test1 = apply_log(train1,test1)\nID_train,train_features,train_labels,ID_test,test_features = get_label_id(train1,test1)\nX_preds,preds1 = train_mod(train_features,test_features,train_labels)\n\ntest_id.extend(list(ID_test))\ntrain_id.extend(list(ID_train))\ntest_pred.extend(list(preds1))\ntrain_pred.extend(list(X_preds))\n\n##p##\ntrain1 = train[train.Tag == 'p']\ntest1 = test[test.Tag == 'p']\ntrain1 = train1.drop(train1[(train1['Views']>2300000) | (train1['Upvotes'] > 160000) | (train1['Answers'] > 39) | (train1['Reputation'] > 600000)].index)\n\ntrain1,test1 = apply_log(train1,test1)\nID_train,train_features,train_labels,ID_test,test_features = get_label_id(train1,test1)\nX_preds,preds1 = train_mod(train_features,test_features,train_labels)\n\ntest_id.extend(list(ID_test))\ntrain_id.extend(list(ID_train))\ntest_pred.extend(list(preds1))\ntrain_pred.extend(list(X_preds))\n\n##s##\ntrain1 = train[train.Tag == 's']\ntest1 = test[test.Tag == 's']\ntrain1 = train1.drop(train1[(train1['Views']>2500000) | (train1['Upvotes'] > 160000) | (train1['Reputation'] > 630000)].index)\n\ntrain1,test1 = apply_log(train1,test1)\nID_train,train_features,train_labels,ID_test,test_features = get_label_id(train1,test1)\nX_preds,preds1 = train_mod(train_features,test_features,train_labels)\n\ntest_id.extend(list(ID_test))\ntrain_id.extend(list(ID_train))\ntest_pred.extend(list(preds1))\ntrain_pred.extend(list(X_preds))\n\n##h##\ntrain1 = train[train.Tag == 'h']\ntest1 = test[test.Tag == 'h']\ntrain1 = train1.drop(train1[(train1['Views']>2200000) | (train1['Upvotes'] > 150000) | (train1['Answers'] > 42) | (train1['Reputation'] > 500000)].index)\n\ntrain1,test1 = apply_log(train1,test1)\nID_train,train_features,train_labels,ID_test,test_features = get_label_id(train1,test1)\nX_preds,preds1 = train_mod(train_features,test_features,train_labels)\n\ntest_id.extend(list(ID_test))\ntrain_id.extend(list(ID_train))\ntest_pred.extend(list(preds1))\ntrain_pred.extend(list(X_preds))\n\n##o##\ntrain1 = train[train.Tag == 'o']\ntest1 = test[test.Tag == 'o']\ntrain1 = train1.drop(train1[(train1['Views']>450000) | (train1['Upvotes'] > 20000) | (train1['Reputation'] > 280000)].index)\n\ntrain1,test1 = apply_log(train1,test1)\nID_train,train_features,train_labels,ID_test,test_features = get_label_id(train1,test1)\nX_preds,preds1 = train_mod(train_features,test_features,train_labels)\n\ntest_id.extend(list(ID_test))\ntrain_id.extend(list(ID_train))\ntest_pred.extend(list(preds1))\ntrain_pred.extend(list(X_preds))\n\n##i##\ntrain1 = train[train.Tag == 'i']\ntest1 = test[test.Tag == 'i']\ntrain1 = train1.drop(train1[(train1['Views']>550000) | (train1['Upvotes'] > 22000) | (train1['Reputation'] > 400000)].index)\n\ntrain1,test1 = apply_log(train1,test1)\nID_train,train_features,train_labels,ID_test,test_features = get_label_id(train1,test1)\nX_preds,preds1 = train_mod(train_features,test_features,train_labels)\n\ntest_id.extend(list(ID_test))\ntrain_id.extend(list(ID_train))\ntest_pred.extend(list(preds1))\ntrain_pred.extend(list(X_preds))\n\n##x##\ntrain1 = train[train.Tag == 'x']\ntest1 = test[test.Tag == 'x']\ntrain1 = train1.drop(train1[(train1['Views']>600000) | (train1['Upvotes'] > 20000) | (train1['Answers'] > 28) | (train1['Reputation'] > 400000)].index)\n\ntrain1,test1 = apply_log(train1,test1)\nID_train,train_features,train_labels,ID_test,test_features = get_label_id(train1,test1)\nX_preds,preds1 = train_mod(train_features,test_features,train_labels)\n\ntest_id.extend(list(ID_test))\ntrain_id.extend(list(ID_train))\ntest_pred.extend(list(preds1))\ntrain_pred.extend(list(X_preds))\n\n##end of all tags##\n\nprint(\"test_id length: \" ,len(test_id))\nprint(\"test_pred length: \" ,len(test_pred))\nprint(\"train_id length: \" ,len(train_id))\nprint(\"train_pred length: \" ,len(train_pred))\n\nsave_off(train_id,train_pred,test_id,test_pred)\nprint('Complete')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a7d173a7ed3151fc67c59815edd8ec90a8a54342"
      },
      "cell_type": "markdown",
      "source": "#xgboost bayesian optimization\nfrom sklearn.cross_validation import cross_val_score\nfrom bayes_opt import BayesianOptimization\n\ntrain2 = train_features.values\nY = train_labels\n\ndef xgboostcv(max_depth,learning_rate,n_estimators,gamma,min_child_weight,subsample,colsample_bytree):\n    return cross_val_score(xgb.XGBRegressor(max_depth=int(max_depth),learning_rate=learning_rate,n_estimators=int(n_estimators),\n                                             silent=True,nthread=-1,gamma=gamma,min_child_weight=min_child_weight,\n                                           subsample=subsample,colsample_bytree=colsample_bytree),\n                           train2,Y,\"neg_mean_squared_error\",cv=5).mean()\n\nxgboostBO = BayesianOptimization(xgboostcv,{'max_depth': (5, 10),'learning_rate': (0.01, 0.3),'n_estimators': (50, 1200),\n                                  'gamma': (0.01,1.0),'min_child_weight': (2, 10),\n                                            'subsample': (0.6, 0.8),'colsample_bytree' :(0.5, 0.99)})\n\nxgboostBO.maximize()\nprint('-'*53)\nprint('Final Results')\nprint('XGBOOST: %f' % xgboostBO.res['max']['max_val'])"
    },
    {
      "metadata": {
        "_uuid": "a2a0217b01a66626a1068183f59d65bba07168ce"
      },
      "cell_type": "markdown",
      "source": "#lightgbm bayesian optimization\nfrom sklearn.cross_validation import cross_val_score\nfrom bayes_opt import BayesianOptimization\n\ntrain2 = train_features.values\nY = train_labels\n\ndef xgboostcv(max_depth,learning_rate,n_estimators,num_leaves,reg_alpha,reg_lambda,subsample,colsample_bytree):\n    return cross_val_score(lgb.LGBMRegressor(max_depth=int(max_depth),learning_rate=learning_rate,n_estimators=int(n_estimators),\n                                             silent=True,nthread=-1,num_leaves=int(num_leaves),reg_alpha=reg_alpha,\n                                           reg_lambda=reg_lambda,subsample=subsample,colsample_bytree=colsample_bytree),\n                           train2,Y,\"r2\",cv=5).mean()\n\nxgboostBO = BayesianOptimization(xgboostcv,{'max_depth': (2, 10),'learning_rate': (0.001, 0.1),'n_estimators': (10, 900),\n                                  'num_leaves': (3,30),'reg_alpha': (1, 5),'reg_lambda': (0, 0.1),\n                                            'subsample': (0.4, 0.8),'colsample_bytree' :(0.4, 0.99)})\n\nxgboostBO.maximize()\nprint('-'*53)\nprint('Final Results')\nprint('XGBOOST: %f' % xgboostBO.res['max']['max_val'])"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}