{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "9d2dbdb3-6c74-4f96-9865-2951dfd653ce",
        "_uuid": "bb41ad86b25fecf332927b0c8f55dd710101e33f"
      },
      "cell_type": "markdown",
      "source": "## BiDirectional LSTM baseline"
    },
    {
      "metadata": {
        "_cell_guid": "2f9b7a76-8625-443d-811f-8f49781aef81",
        "_uuid": "598f965bc881cfe6605d92903b758778d400fa8b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras import backend as K\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,CuDNNLSTM\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\nfrom nltk import sent_tokenize",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "c297fa80-beea-464b-ac90-f380ebdb02fe",
        "_uuid": "d961885dfde18796893922f72ade1bf64456404e"
      },
      "cell_type": "markdown",
      "source": "We include the GloVe word vectors in our input files. To include these in your kernel, simple click 'input files' at the top of the notebook, and search 'glove' in the 'datasets' section."
    },
    {
      "metadata": {
        "_cell_guid": "66a6b5fd-93f0-4f95-ad62-3253815059ba",
        "_uuid": "729b0f0c2a02c678631b8c072d62ff46146a82ef",
        "trusted": true
      },
      "cell_type": "code",
      "source": "EMBEDDING_FILE='../input/glove6b50d/glove.6B.50d.txt'",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9afd9451058e4a636bb4716077859e5a4a17033f"
      },
      "cell_type": "code",
      "source": "test = pd.read_csv('../input/he-accenture/test.csv')\ntrain = pd.read_csv('../input/he-accenture/train.csv')",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "37164a2d565d7478e8ab8a9fef3c7cc074eaf6c4"
      },
      "cell_type": "code",
      "source": "train.head()",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "    UID  ...  score\n0  Tr-1  ...      2\n1  Tr-2  ...     -4\n2  Tr-3  ...      3\n3  Tr-4  ...     -8\n4  Tr-5  ...      6\n\n[5 rows x 5 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UID</th>\n      <th>comment</th>\n      <th>date</th>\n      <th>parent_comment</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Tr-1</td>\n      <td>NC and NH.</td>\n      <td>2016-10</td>\n      <td>Yeah, I get that argument. At this point, I'd ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Tr-2</td>\n      <td>You do know west teams play against west teams...</td>\n      <td>2016-11</td>\n      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n      <td>-4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Tr-3</td>\n      <td>They were underdogs earlier today, but since G...</td>\n      <td>2016-09</td>\n      <td>They're favored to win.</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Tr-4</td>\n      <td>This meme isn't funny none of the \"new york ni...</td>\n      <td>2016-10</td>\n      <td>deadass don't kill my buzz</td>\n      <td>-8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Tr-5</td>\n      <td>I could use one of those tools.</td>\n      <td>2016-12</td>\n      <td>Yep can confirm I saw the tool they use for th...</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "98f2b724-7d97-4da8-8b22-52164463a942",
        "_uuid": "b62d39216c8d00b3e6b78b825212fd190757dff9"
      },
      "cell_type": "markdown",
      "source": "Set some basic config parameters:"
    },
    {
      "metadata": {
        "_cell_guid": "2807a0a5-2220-4af6-92d6-4a7100307de2",
        "_uuid": "d365d5f8d9292bb9bf57d21d6186f8b619cbe8c3",
        "trusted": true
      },
      "cell_type": "code",
      "source": "embed_size = 50\nmax_features = 20000\nmaxlen = 100",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b3a8d783-95c2-4819-9897-1320e3295183",
        "_uuid": "4dd8a02e7ef983f10ec9315721c6dda2958024af"
      },
      "cell_type": "markdown",
      "source": "Read in our data and replace missing values:"
    },
    {
      "metadata": {
        "_cell_guid": "ac2e165b-1f6e-4e69-8acf-5ad7674fafc3",
        "_uuid": "8ab6dad952c65e9afcf16e43c4043179ef288780",
        "trusted": true
      },
      "cell_type": "code",
      "source": "list_sentences_train = train[\"parent_comment\"].fillna(\"_na_\").values\ny = train['score'].values\nlist_sentences_test = test[\"parent_comment\"].fillna(\"_na_\").values",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79afc0e9-b5f0-42a2-9257-a72458e91dbb",
        "_uuid": "c292c2830522bfe59d281ecac19f3a9415c07155",
        "trusted": true
      },
      "cell_type": "code",
      "source": "tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7d19392b-7750-4a1b-ac30-ed75b8a62d52",
        "_uuid": "e9e3b4fa7c4658e0f22dd48cb1a289d9deb745fc",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4d29d827-377d-4d2f-8582-4a92f9569719",
        "_uuid": "96fc33012e7f07a2169a150c61574858d49a561b",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nemb_mean,emb_std",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  \"\"\"Entry point for launching an IPython kernel.\n",
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "(0.020940498, 0.6441043)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "62acac54-0495-4a26-ab63-2520d05b3e19",
        "_uuid": "574c91e270add444a7bc8175440274bdd83b7173",
        "trusted": true
      },
      "cell_type": "code",
      "source": "word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "eded28dcbbfc4b6dfa297afd60e58ad452b47f7a"
      },
      "cell_type": "code",
      "source": "embedding_matrix[0]",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "array([-0.47107019, -1.10401832,  1.29062063,  0.89376371,  0.40853162,\n        0.27875739, -0.69818692,  0.62945259,  0.87593289,  0.57920566,\n        0.17053255, -0.34860669,  0.29394367,  0.96782405, -1.47327159,\n        0.37755123, -0.99562403,  0.81564513,  0.90965464,  0.33451224,\n       -0.24781101, -0.7812317 , -0.34947127,  0.02051574, -0.65041278,\n       -0.17706355, -0.21324179,  0.34074742,  0.8865554 , -0.83005841,\n        0.40199546,  0.16029716, -0.35504682,  0.47784263, -1.21908015,\n        0.47664462,  0.47666782, -0.08967921, -0.34191007, -0.40728989,\n        0.19196549, -0.55590391,  0.2833674 ,  0.43772412, -0.06909767,\n        0.06685626,  0.48040836, -0.20854702,  0.67613445,  0.02588322])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7b7f4df6f79c48436ec8f336c04dc96ae49315d8"
      },
      "cell_type": "code",
      "source": "def root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0d4cb718-7f9a-4eab-acda-8f55b4712439",
        "_uuid": "dc51af0bd046e1eccc29111a8e2d77bdf7c60d28",
        "trusted": true
      },
      "cell_type": "code",
      "source": "inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(CuDNNLSTM(50, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "333626f1-a838-4fea-af99-0c78f1ef5f5c",
        "scrolled": false,
        "_uuid": "c1558c6b2802fc632edc4510c074555a590efbd8",
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.fit(X_t, y, batch_size=32, epochs=3, validation_split=0.1);",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 40500 samples, validate on 4500 samples\nEpoch 1/3\n40500/40500 [==============================] - 59s 1ms/step - loss: 2439.0314 - acc: 0.3427 - val_loss: 1475.4313 - val_acc: 0.3382\nEpoch 2/3\n40500/40500 [==============================] - 57s 1ms/step - loss: 2438.9404 - acc: 0.3434 - val_loss: 1475.4313 - val_acc: 0.3382\nEpoch 3/3\n40500/40500 [==============================] - 57s 1ms/step - loss: 2438.9404 - acc: 0.3434 - val_loss: 1475.4313 - val_acc: 0.3382\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "d6fa2ace-aa92-40cf-913f-a8f5d5a4b130",
        "_uuid": "3dbaa4d0c22271b8b0dc7e58bcad89ddc607beaf"
      },
      "cell_type": "markdown",
      "source": "And finally, get predictions for the test set and prepare a submission CSV:"
    },
    {
      "metadata": {
        "_cell_guid": "617e974a-57ee-436e-8484-0fb362306db2",
        "_uuid": "2b969bab77ab952ecd5abf2abe2596a0e23df251",
        "trusted": true
      },
      "cell_type": "code",
      "source": "prediction = model.predict([X_te])",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e1373f6000dbba9f818ba66cfa49298f96683a2"
      },
      "cell_type": "code",
      "source": "# prediction[:50]",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "09fdfc2d1454ae17e2c39a353f28ea1e28c7ff97"
      },
      "cell_type": "code",
      "source": "submission = pd.DataFrame.from_dict({'UID': test['UID']})\nsubmission['score'] = prediction\nsubmission.to_csv('submission.csv', index=False)",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "293c8aa8b4c4a39ec3401dc3d9ac51deddfcff35"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}