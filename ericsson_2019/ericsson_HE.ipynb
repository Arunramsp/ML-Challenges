{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport random\nimport xgboost as xgb\n\nfrom sklearn import preprocessing\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom math import sqrt\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import matthews_corrcoef\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, average_precision_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nfrom tqdm import tqdm_notebook\nInteractiveShell.ast_node_interactivity = \"all\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# download flair library #\nimport torch\n!pip install flair\nimport flair","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"os.listdir('../input/nlp_datac2476d7/NLP_Data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/predictive_data32f5357/Predictive_Data/train_file.csv')\ntest = pd.read_csv('../input/predictive_data32f5357/Predictive_Data/test_file.csv')\nsubm = pd.read_csv('../input/predictive_data32f5357/Predictive_Data/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['UsageClass','CheckoutType','CheckoutYear','CheckoutMonth'],axis=1)\ntest = test.drop(['UsageClass','CheckoutType','CheckoutYear','CheckoutMonth'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('class count in numbers: ')\ntrain['MaterialType'].value_counts()\nprint('percentage of class count : ')\ntrain['MaterialType'].value_counts()/train.shape[0] * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of NaNs in each rows\ntrain.isnull().sum(axis=1).head(5)\ntrain.isnull().sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.nunique()\ntest.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(train['PublicationYear'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['PublicationYear'].replace(np.nan,\"0000\",inplace=True)\ntest['PublicationYear'].replace(np.nan,\"0000\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Subjects'].replace(np.nan,\"\",inplace=True)\ntest['Subjects'].replace(np.nan,\"\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef matc(a):\n    return re.findall(r\"[0-9]{4}\",a)\ntrain['PublicationYear'] = train['PublicationYear'].apply(lambda x: matc(x))\ntest['PublicationYear'] = test['PublicationYear'].apply(lambda x: matc(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['combined'] = train['Title'] + ' . ' + train['Subjects'] + ' . ' + train['Creator'] + ' . ' + train['Publisher']\ntest['combined'] = test['Title'] + ' . ' + test['Subjects'] + ' . ' + test['Creator'] + ' . ' + test['Publisher']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ext(a):\n    #print(a)\n    if len(a)==0:\n        return 0\n    #print(int(a[0]))\n    return int(a[0])\ntrain['PublicationYear'] = train['PublicationYear'].apply(lambda x:ext(x))\ntest['PublicationYear'] = test['PublicationYear'].apply(lambda x:ext(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set(train['MaterialType'])\n\ntrain['MaterialType'].replace(\"BOOK\",0,inplace=True)\ntrain['MaterialType'].replace(\"CR\",1,inplace=True)\ntrain['MaterialType'].replace(\"MIXED\",2,inplace=True)\ntrain['MaterialType'].replace(\"MUSIC\",3,inplace=True)\ntrain['MaterialType'].replace(\"SOUNDCASS\",4,inplace=True)\ntrain['MaterialType'].replace(\"SOUNDDISC\",5,inplace=True)\ntrain['MaterialType'].replace(\"VIDEOCASS\",6,inplace=True)\ntrain['MaterialType'].replace(\"VIDEODISC\",7,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Publisher'].replace(np.nan,\"\",inplace=True)\ntest['Publisher'].replace(np.nan,\"\",inplace=True)\n\ntrain['Creator'].replace(np.nan,\"\",inplace=True)\ntest['Creator'].replace(np.nan,\"\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tfidf"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n# TF-IDF feature matrix\ntrain1 = tfidf_vectorizer.fit_transform(train['combined'])\ntest1 = tfidf_vectorizer.fit_transform(test['combined'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['combined'].shape\ntest['combined'].shape\n\nnp.hstack([train['combined'].values,test['combined'].values]).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n# create the transform\nvectorizer = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n# tokenize and build vocab\nvectorizer.fit(np.hstack([train['combined'].values,test['combined'].values]))\ntrain2 = vectorizer.transform(train['combined'])\ntest2 = vectorizer.transform(test['combined'])\nprint(train2.shape)\nprint(type(train2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2\ntest2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csr_matrix([train['Checkouts']]).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack, coo_matrix, csr_matrix,vstack\ntrain3 = hstack((train1, train2))\n\ntest3 = hstack((test1, test2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train3\ntest3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n# splitting data into training and validation set\nxtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train3, train['MaterialType'], random_state=42, test_size=0.2)\n\n# lreg = LogisticRegression()\nlreg = lgb.LGBMRegressor()\nlreg.fit(xtrain_bow, ytrain) # training the model\n\nprediction = lreg.predict(xvalid_bow) # predicting on the validation set\n# prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\nprediction_int = prediction.astype(np.int)\n\nf1_score(yvalid, prediction_int,average='micro') # calculating f1 score\n\ntest_pred = lreg.predict(test3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n\n# splitting data into training and validation set\nxtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train3, train['MaterialType'], random_state=42, test_size=0.2)\n\nlreg = LogisticRegression()\n# lreg = lgb.LGBMRegressor()\nlreg.fit(xtrain_bow, ytrain) # training the model\n\nprediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\nprediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\nprediction_int = prediction_int.astype(np.int)\n\nf1_score(yvalid, prediction_int,average='micro') # calculating f1 score\n\ntest_pred = lreg.predict_proba(test3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB(alpha=0.8)\nnb.fit(xtrain_bow, ytrain)\n\npreds = nb.predict(xvalid_bow)\nf1_score(yvalid, preds,average='micro') # calculating f1 score\n\ntest_pred = nb.predict(test3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(test_pred.astype(np.int))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set(prediction_int)\n# max(prediction[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subfin = pd.DataFrame({'ID': test['ID'].values, 'MaterialType': test_pred})\nsubfin=subfin.reindex(columns=[\"ID\",\"MaterialType\"])\nsubfin.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subfin.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subfin['MaterialType'].replace(0,\"BOOK\",inplace=True)\nsubfin['MaterialType'].replace(1,\"CR\",inplace=True)\nsubfin['MaterialType'].replace(2,\"MIXED\",inplace=True)\nsubfin['MaterialType'].replace(3,\"MUSIC\",inplace=True)\nsubfin['MaterialType'].replace(4,\"SOUNDCASS\",inplace=True)\nsubfin['MaterialType'].replace(5,\"SOUNDDISC\",inplace=True)\nsubfin['MaterialType'].replace(6,\"VIDEOCASS\",inplace=True)\nsubfin['MaterialType'].replace(7,\"VIDEODISC\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subfin.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"submission.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe\ncreate_download_link(subfin)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Flair"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\ntrain['Publisher'] = le.fit_transform(train['Publisher'])\ntest['Publisher'] = le.transform(test['Publisher'])\n\ntrain['Creator'] = le.fit_transform(train['Creator'])\ntest['Creator'] = le.transform(test['Creator'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"set(train['Creator']) - set(test['Creator'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from flair.data import Sentence\n# create a sentence #\nsentence = Sentence('Awesomeness come from the core.')\n# print the sentence to see what’s in it. #\nprint(Sentence.get_embedding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracting the tweet part#\ntext = train['combined'] \n ## txt is a list of tweets ##\ntxt = text.tolist()\nprint(txt[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Importing the Embeddings ##\nfrom flair.embeddings import WordEmbeddings\nfrom flair.embeddings import CharacterEmbeddings\nfrom flair.embeddings import StackedEmbeddings\nfrom flair.embeddings import FlairEmbeddings\nfrom flair.embeddings import BertEmbeddings\nfrom flair.embeddings import ELMoEmbeddings\nfrom flair.embeddings import FlairEmbeddings\n\n### Initialising embeddings (un-comment to use others) ###\n#glove_embedding = WordEmbeddings('glove')\n#character_embeddings = CharacterEmbeddings()\nflair_forward  = FlairEmbeddings('news-forward-fast')\nflair_backward = FlairEmbeddings('news-backward-fast')\n#bert_embedding = BertEmbedding()\n#elmo_embedding = ElmoEmbedding()\n\nstacked_embeddings = StackedEmbeddings( embeddings = [ \n                                                       flair_forward, \n                                                       flair_backward\n                                                      ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a sentence #\nsentence = Sentence(\"Awesomeness come from the core.\")\n# embed words in sentence #\nstacked_embeddings.embed(sentence)\nfor token in sentence:\n      print(token.embedding)\n# data type and size of embedding #\nprint(type(token.embedding))\n# storing size (length) #\nz = token.embedding.size()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from flair.embeddings import DocumentPoolEmbeddings\nfrom tqdm import tqdm\n### initialize the document embeddings, mode = mean ###\ndocument_embeddings = DocumentPoolEmbeddings([flair_backward, flair_forward ])\ns = torch.zeros(0,z)\n# iterating Sentences #\nfor tweet in tqdm(txt):\n        #print(tweet)\n        sentence = Sentence(tweet)\n        document_embeddings.embed(sentence)\n        # Storing Size of embedding #\n        z = sentence.embedding.size()[0]\n        # Adding Document embeddings to list #\n        s = torch.cat((s, sentence.embedding.view(-1,z)),0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# s = torch.cat((s,torch.zeros(2048).view(-1,2048)),0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## tensor to numpy array ##\nX = s.numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('train_embed',np.column_stack((X, train['Checkouts'],train['PublicationYear'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracting the tweet part#\ntext = test['combined'] \ntxt = text.tolist()\n\nfrom flair.embeddings import WordEmbeddings\nfrom flair.embeddings import CharacterEmbeddings\nfrom flair.embeddings import StackedEmbeddings\nfrom flair.embeddings import FlairEmbeddings\nfrom flair.embeddings import BertEmbeddings\nfrom flair.embeddings import ELMoEmbeddings\nfrom flair.embeddings import FlairEmbeddings\n\nflair_forward  = FlairEmbeddings('news-forward-fast')\nflair_backward = FlairEmbeddings('news-backward-fast')\n\nstacked_embeddings = StackedEmbeddings( embeddings = [ flair_forward, flair_backward])\n\n# create a sentence #\nsentence = Sentence(\"Analytics Vidhya blogs are Awesome.\")\n# embed words in sentence #\nstacked_embeddings.embed(sentence)\nfor token in sentence:\n      print(token.embedding)\n# storing size (length) #\nz = token.embedding.size()[0]\n\nfrom flair.embeddings import DocumentPoolEmbeddings\nfrom tqdm import tqdm\n### initialize the document embeddings, mode = mean ###\ndocument_embeddings = DocumentPoolEmbeddings([flair_backward,flair_forward])\ns = torch.zeros(0,z)\n# iterating Sentences #\nfor tweet in tqdm(txt):\n        sentence = Sentence(tweet)\n        document_embeddings.embed(sentence)\n        # Storing Size of embedding #\n        z = sentence.embedding.size()[0]\n        # Adding Document embeddings to list #\n        s = torch.cat((s, sentence.embedding.view(-1,z)),0)\nX1 = s.numpy()\nnp.save('test_embed',np.column_stack((X1, test['Checkouts'],test['PublicationYear'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = np.column_stack((X, train['Checkouts'],train['PublicationYear']))\ntest1 = np.column_stack((X1, test['Checkouts'],test['PublicationYear']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train['MaterialType'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n### Splitting training set ###\nx_train, x_valid, y_train, y_valid = train_test_split(train1, target,random_state=42,test_size=0.2)\n\n### XGBoost compatible data ###\ndtrain = xgb.DMatrix(x_train,y_train)         \ndvalid = xgb.DMatrix(x_valid, label = y_valid)\n\n### defining parameters ###\nparams = {\n          'colsample': 0.9,\n          'colsample_bytree': 0.5,\n          'eta': 0.1,\n          'max_depth': 8,\n          'min_child_weight': 6,\n          'objective': 'multi:softmax',\n          'subsample': 0.9,\n        'num_class':8\n          }\n\n### Training the model ###\nxgb_model = xgb.train(\n                      params,\n                      dtrain,\n                      feval= custom_eval,\n                      num_boost_round= 1000,\n                      maximize=True,\n                      evals=[(dvalid, \"Validation\")],\n                      early_stopping_rounds=30\n                      )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# params = {'task': 'train',\n#     'boosting_type': 'gbdt',\n#     'objective': 'multiclass',\n#     'num_class':8,\n#     'metric': 'custom_eval',\n#     'learning_rate': 0.002296,\n#     'max_depth': 7,\n#     'num_leaves': 17,\n#     'metric': ['multi_error'],\n#     'feature_fraction': 0.4,\n#     'bagging_fraction': 0.6,\n#     'bagging_freq': 17}\n\nimport lightgbm as lgb\n### Splitting training set ###\nx_train, x_valid, y_train, y_valid = train_test_split(train1, target,random_state=42,test_size=0.2)\nprint(x_train.shape)\nd_train = lgb.Dataset(x_train, label=y_train)\nparams = {}\n# params['learning_rate'] = 0.003\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'multiclass'\nparams['num_class'] = 8\nparams['metric'] = ['multi_error']\n# params['sub_feature'] = 0.5\n# params['num_leaves'] = 10\n# params['min_data'] = 50\n# params['max_depth'] = 10\nparams['silent'] = 2\nclf = lgb.train(params, d_train, 2000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'done'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=clf.predict(test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_pred.shape\nset(np.argmax(y_pred,axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_eval(preds, dtrain):\n    labels = dtrain.get_label().astype(np.int)\n    preds = (preds >= 0.3).astype(np.int)\n    return [('f1_score', f1_score(labels, preds, average=None))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import tensorflow_hub as hub\n# import tensorflow as tf\n\n# elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # just a random sentence\n# x = [\"Roasted ants are a popular snack in Columbia\"]\n\n# # Extract ELMo features \n# embeddings = elmo(x, signature=\"default\", as_dict=True)[\"elmo\"]\n\n# embeddings.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# embeddings.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def elmo_vectors(x):\n#   embeddings = elmo(x.tolist(), signature=\"default\", as_dict=True)[\"elmo\"]\n\n#   with tf.Session() as sess:\n#     sess.run(tf.global_variables_initializer())\n#     sess.run(tf.tables_initializer())\n#     # return average of ELMo features\n#     return sess.run(tf.reduce_mean(embeddings,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list_train = [train[i:i+100] for i in range(0,train.shape[0],100)]\n# list_test = [test[i:i+100] for i in range(0,test.shape[0],100)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# # Extract ELMo embeddings\n# elmo_train = [elmo_vectors(x['Subjects']) for x in list_train]\n# elmo_test = [elmo_vectors(x['Subjects']) for x in list_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 2nd Problem"},{"metadata":{"trusted":true},"cell_type":"code","source":"nl_train = pd.read_csv('../input/nlp_datac2476d7/NLP_Data/train.csv')\nnl_test = pd.read_csv('../input/nlp_datac2476d7/NLP_Data/test.csv')\nnl_subm = pd.read_csv('../input/nlp_datac2476d7/NLP_Data/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nl_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nl_test.index[nl_test['date']==' Jan 0, 0000']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### changing the wrong date values to default date"},{"metadata":{"trusted":true},"cell_type":"code","source":"nl_test['date'].replace('None',\" Jan 1, 2000\",inplace=True)\nnl_test['date'].replace(' Nov 0, 0000',\" Jan 1, 2000\",inplace=True)\nnl_test['date'].replace(' Jan 0, 0000',\" Jan 1, 2000\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### create features like day,month & year from the date."},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\ndef convert_date(d):\n    return datetime.strptime(d,' %b %d, %Y')\n# datetime.strptime(nl_train['date'][0], ' %b %d, %Y').date().day\nnl_train['day'] = nl_train['date'].apply(lambda x: convert_date(x).date().day)\nnl_train['month'] = nl_train['date'].apply(lambda x: convert_date(x).date().month)\nnl_train['year'] = nl_train['date'].apply(lambda x: convert_date(x).date().year)\n\nnl_test['day'] = nl_test['date'].apply(lambda x: convert_date(x).date().day)\nnl_test['month'] = nl_test['date'].apply(lambda x: convert_date(x).date().month)\nnl_test['year'] = nl_test['date'].apply(lambda x: convert_date(x).date().year)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"pd.to_datetime(nl_train.date.str, format='%b %d,%Y', yearfirst=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### finding the frequency of target classes"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print('class count in numbers: ')\nnl_train['overall'].value_counts()\nprint('percentage of class count : ')\nnl_train['overall'].value_counts()/train.shape[0] * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nl_train.nunique()\nnl_test.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of NaNs in each rows\nnl_train.isnull().sum(axis=0)\nnl_test.isnull().sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### label encode the text fields"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(nl_train['Place'])\nnl_train['Place'] = le.transform(nl_train['Place'])\nnl_test['Place'] = le.transform(nl_test['Place'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nle.fit(nl_train['status'])\nnl_train['status'] = le.transform(nl_train['status'])\nnl_test['status'] = le.transform(nl_test['status'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nl_test['negatives'].replace(np.nan,\"\",inplace=True)\n\nnl_train['advice_to_mgmt'].replace(np.nan,\"\",inplace=True)\nnl_test['advice_to_mgmt'].replace(np.nan,\"\",inplace=True)\n\nnl_train['summary'].replace(np.nan,\"\",inplace=True)\nnl_test['summary'].replace(np.nan,\"\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### combine all the text column into one"},{"metadata":{"trusted":true},"cell_type":"code","source":"nl_train['combined'] = nl_train['summary'] + ' . ' + nl_train['positives'] + ' . ' + nl_train['negatives'] + ' . ' +  nl_train['advice_to_mgmt']\nnl_test['combined'] = nl_test['summary'] + ' . ' + nl_test['positives'] + ' . ' + nl_test['negatives'] + ' . ' +  nl_test['advice_to_mgmt']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n# TF-IDF feature matrix\ntrain1 = tfidf_vectorizer.fit_transform(nl_train['combined'])\ntest1 = tfidf_vectorizer.fit_transform(nl_test['combined'])\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n# create the transform\nvectorizer = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n# tokenize and build vocab\nvectorizer.fit(np.hstack([nl_train['combined'].values,nl_test['combined'].values]))\nprint(train2.shape)\nprint(type(train2))\ntrain2 = vectorizer.transform(nl_train['combined'])\ntest2 = vectorizer.transform(nl_test['combined'])\n\nfrom scipy.sparse import hstack, csr_matrix,vstack\ntrain3 = hstack((train1, train2))\ntest3 = hstack((test1, test2))\n\ntrain3\ntest3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB(alpha=0.8)\n# splitting data into training and validation set\nxtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train3, nl_train['overall'], random_state=42, test_size=0.2)\n\nnb.fit(xtrain_bow, ytrain)\n\npreds = nb.predict(xvalid_bow)\nf1_score(yvalid, preds,average='micro') # calculating f1 score\ntest_pred = nb.predict(test3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### create more features"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef get_polarity(txt):\n    return TextBlob(txt).sentiment.polarity\nnl_train['summ_score'] = nl_train['summary'].apply(lambda x:get_polarity(x))\nnl_train['pos_score'] = nl_train['positives'].apply(lambda x:get_polarity(x))\nnl_train['neg_score'] = nl_train['negatives'].apply(lambda x:get_polarity(x))\nnl_train['adv_score'] = nl_train['advice_to_mgmt'].apply(lambda x:get_polarity(x))\n\nnl_test['summ_score'] = nl_test['summary'].apply(lambda x:get_polarity(x))\nnl_test['pos_score'] = nl_test['positives'].apply(lambda x:get_polarity(x))\nnl_test['neg_score'] = nl_test['negatives'].apply(lambda x:get_polarity(x))\nnl_test['adv_score'] = nl_test['advice_to_mgmt'].apply(lambda x:get_polarity(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nl_train['score_1'].replace(np.nan,nl_train['score_1'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column = ['Place','status','score_1','score_2','score_3','score_4','score_5','score_6','day','month','year']\n#'summ_score','pos_score','neg_score','adv_score'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nl_train['score_1'].replace(np.nan,nl_train['score_1'].mean(),inplace=True)\nnl_train['score_2'].replace(np.nan,nl_train['score_2'].mean(),inplace=True)\nnl_train['score_3'].replace(np.nan,nl_train['score_3'].mean(),inplace=True)\nnl_train['score_4'].replace(np.nan,nl_train['score_4'].mean(),inplace=True)\nnl_train['score_5'].replace(np.nan,nl_train['score_5'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB(alpha=0.8)\n\n# splitting data into training and validation set\nxtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(nl_train[column], nl_train['overall'], random_state=42, test_size=0.2)\n\nnb.fit(xtrain_bow, ytrain)\n\npreds = nb.predict(xvalid_bow)\nf1_score(yvalid, preds,average='micro') # calculating f1 score\ntest_pred = nb.predict(nl_test[column])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n# splitting data into training and validation set\n# xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(nl_train[column], nl_train['overall'], random_state=42, test_size=0.2)\nxtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train3, nl_train['overall'], random_state=42, test_size=0.2)\n\n# lreg = LogisticRegression()\nlreg = lgb.LGBMRegressor()\nlreg.fit(xtrain_bow, ytrain) # training the model\n\nprediction = lreg.predict(xvalid_bow) # predicting on the validation set\n# prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\nprediction_int = prediction.astype(np.int)\n\nf1_score(yvalid, prediction_int,average='micro') # calculating f1 score\n\ntest_pred = lreg.predict(test3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(test_pred.astype(np.int))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subfin = pd.DataFrame({'ID': nl_test['ID'].values, 'overall': test_pred.astype(np.int)})\nsubfin=subfin.reindex(columns=[\"ID\",\"overall\"])\nsubfin.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"submission.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe\ncreate_download_link(subfin)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}