{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%%time\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport random\nimport xgboost as xgb\n\nfrom sklearn import preprocessing\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom math import sqrt\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.metrics import matthews_corrcoef\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, average_precision_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.models import Model\nfrom keras.layers import Dense, Embedding, Input, Concatenate, Conv1D, Activation, TimeDistributed, Flatten, RepeatVector, Permute,multiply,GlobalAveragePooling1D, GlobalMaxPooling1D ,CuDNNGRU\nfrom keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, GRU, GlobalAveragePooling1D, MaxPooling1D, SpatialDropout1D, BatchNormalization, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import Callback\nfrom keras.models import load_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import Callback\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nfrom tqdm import tqdm_notebook\nInteractiveShell.ast_node_interactivity = \"all\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer=TweetTokenizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir('../input/glove840b300dtxt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/ericsson-data/predictive_data32f5357/Predictive_Data/train_file.csv')\ntest = pd.read_csv('../input/ericsson-data/predictive_data32f5357/Predictive_Data/test_file.csv')\nsubm = pd.read_csv('../input/ericsson-data/predictive_data32f5357/Predictive_Data/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### These having only one value so we are removing that"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['UsageClass','CheckoutType','CheckoutYear','CheckoutMonth'],axis=1)\ntest = test.drop(['UsageClass','CheckoutType','CheckoutYear','CheckoutMonth'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('class count in numbers: ')\ntrain['MaterialType'].value_counts()\nprint('percentage of class count : ')\ntrain['MaterialType'].value_counts()/train.shape[0] * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### encode the target vriable"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['MaterialType'].replace(\"BOOK\",0,inplace=True)\ntrain['MaterialType'].replace(\"CR\",1,inplace=True)\ntrain['MaterialType'].replace(\"MIXED\",2,inplace=True)\ntrain['MaterialType'].replace(\"MUSIC\",3,inplace=True)\ntrain['MaterialType'].replace(\"SOUNDCASS\",4,inplace=True)\ntrain['MaterialType'].replace(\"SOUNDDISC\",5,inplace=True)\ntrain['MaterialType'].replace(\"VIDEOCASS\",6,inplace=True)\ntrain['MaterialType'].replace(\"VIDEODISC\",7,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## used to find out number of unique variables in each column\ntrain.nunique()\ntest.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## impute the nan values\ntrain['PublicationYear'].replace(np.nan,\"0000\",inplace=True)\ntest['PublicationYear'].replace(np.nan,\"0000\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Subjects'].replace(np.nan,\"\",inplace=True)\ntest['Subjects'].replace(np.nan,\"\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## clean the PublicationYear column\nimport re\ndef matc(a):\n    return re.findall(r\"[0-9]{4}\",a)\ntrain['PublicationYear'] = train['PublicationYear'].apply(lambda x: matc(x))\ntest['PublicationYear'] = test['PublicationYear'].apply(lambda x: matc(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Creator'].replace(np.nan,\"\",inplace=True)\ntrain['Publisher'].replace(np.nan,\"\",inplace=True)\n\ntest['Creator'].replace(np.nan,\"\",inplace=True)\ntest['Publisher'].replace(np.nan,\"\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Combine all the Text columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['combined'] = train['Title'] + ' . ' + train['Subjects'] + ' . ' + train['Creator'] + ' . ' + train['Publisher']\ntest['combined'] = test['Title'] + ' . ' + test['Subjects'] + ' . ' + test['Creator'] + ' . ' + test['Publisher']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['combined'].replace(np.nan,\"\",inplace=True)\ntest['combined'].replace(np.nan,\"\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ext(a):\n    #print(a)\n    if len(a)==0:\n        return 0\n    #print(int(a[0]))\n    return int(a[0])\ntrain['PublicationYear'] = train['PublicationYear'].apply(lambda x:ext(x))\ntest['PublicationYear'] = test['PublicationYear'].apply(lambda x:ext(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text Cleaning before computing embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"merge=pd.concat([train.iloc[:,8:9],test.iloc[:,8:9]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# corpus = train['combined']\ncorpus = merge['combined'].str.lower()\ndef isEnglish(s):\n    try:\n        s.encode(encoding='utf-8').decode('ascii')\n    except UnicodeDecodeError:\n        return False\n    else:\n        return True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nThis function receives comments and returns clean word-list\n\"\"\"\ndef clean(comment):\n    #Convert to lower case , so that Hi and hi are the same\n    #comment=comment.lower()\n    #remove \\n\n    comment=re.sub(\"\\\\n\",\" \",comment)\n    #removing usernames\n    comment=re.sub(\"\\[\\[.*\\]\",\" \",comment)\n    #remove hyperlink\n    comment=re.sub(\"http\\S+|www.\\S+\",\" \",comment)\n    #remove numbers\n    comment = clearup(comment, string.punctuation+string.digits)\n    #Split the sentences into words\n    words=tokenizer.tokenize(comment)\n    # (')aphostophe  replacement (ie)   you're --> you are  \n    # ( basic dictionary lookup : master dictionary present in a hidden block of code)\n    words=[APPO[word] if word in APPO else word for word in words]\n    #other commonly misspeled words.\n    words=[repl[word] if word in replkeys else word for word in words]\n    clean_sent=\" \".join(words)\n    # remove any non alphanum,digit character\n    clean_sent=re.sub(\"\\W+\",\" \",clean_sent)\n    # remove the punctuations from the text\n    remove_punctuations(clean_sent)\n    clean_sent=re.sub(\"  \",\" \",clean_sent)\n    clean_sent=clean_sent.strip()\n    return(clean_sent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclean_corpus=corpus.apply(lambda x :clean(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('loading embeddings vectors')\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using Glove Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nembeddings_index = dict(get_coefs(*o.split(' ')) for o in open('../input/glove840b300dtxt/glove.840B.300d.txt'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list_sentences_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features=150000\nmaxlen=200\n\nlist_sentences_train = train[\"combined\"].str.lower()\nlist_classes = [\"MaterialType\"]\n\ntrain[list_classes] = train[list_classes].astype(np.int8)\ntarget = train[list_classes]\n\nlist_sentences_test = test[\"combined\"]\ntrb_nan_idx = list_sentences_test[pd.isnull(list_sentences_test)].index.tolist()\nlist_sentences_test.loc[trb_nan_idx] = ' '\nlist_sentences_test = list_sentences_test.str.lower()\n\nprint('mean text len:',train[\"combined\"].str.count('\\S+').mean())\nprint('max text len:',train[\"combined\"].str.count('\\S+').max())\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train) + list(list_sentences_test))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nprint('padding sequences')\nX_train = train[\"combined\"]\nX_test = test[\"combined\"]\nX_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\nX_train = np.array(X_train)\nX_test = np.array(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[list_classes] = train[list_classes].astype(np.int8)\ntarget = train[list_classes]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('numerical variables')\nembed_size=300\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nprint('create embedding matrix')\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n#embedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i == 51688:\n        continue\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: \n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# embedding_matrix[51687]\nlen(word_index.values())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### one hot encode the target variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(handle_unknown='ignore')\ntarget = enc.fit_transform(target).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = pd.DataFrame(data=target[0:,0:], index=list(range(0,len(target))), columns=target[0,0:]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape\ntarget.shape\nX_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### model used is the single Gru Cnn model with concatenating the max_pool and avg_pool layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_single_grucnn_model():\n    \n    inp = Input(shape=(200, ))\n    x = Embedding(51688, 300, weights=[embedding_matrix], trainable = False)(inp)\n    x = SpatialDropout1D(0.3)(x)\n    x2 = Bidirectional(CuDNNGRU(80, kernel_initializer='glorot_uniform', return_sequences=True))(x)\n    x2 = Conv1D(120, kernel_size = 2, padding = \"valid\",activation = \"relu\",strides = 1)(x2)\n    #x = Dropout(0.2)(x)\n    max_pool = GlobalMaxPooling1D()(x2)\n    avg_pool = GlobalAveragePooling1D()(x2)\n    x = Concatenate()([avg_pool,max_pool])\n    x = Dense(8, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    print(model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('start modeling')\nearly_stop = EarlyStopping(monitor = \"loss\", mode = \"min\", patience = 5)\nscores = []\npredict = np.zeros((test.shape[0],8))\noof_predict = np.zeros((train.shape[0],8))\n\nprint(\"model selection\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### two fold cross-validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_folds = 2\nkf = KFold(n_splits=num_folds, shuffle=True, random_state=239)\nfor train_index, test_index in kf.split(X_train):\n    X_train1 , X_valid = X_train[train_index] , X_train[test_index]\n    y_train, y_val = target.loc[train_index], target.loc[test_index]\n    model = get_single_grucnn_model()\n    model.fit(X_train1, y_train, batch_size=256, epochs=10, verbose=1,callbacks = [early_stop])\n    print('Predicting....')\n    oof_predict[test_index] = model.predict(X_valid, batch_size=1024)\n#     cv_score = roc_auc_score(y_test, oof_predict[test_index])\n    #scores.append(cv_score)\n    #print('score: ',cv_score)\n    print('pridicting test')\n    predict += model.predict(X_test, batch_size=1024)\n\npredict = predict / num_folds\n#print('Total CV score is {}'.format(np.mean(scores)))\nsample_submission = pd.DataFrame.from_dict({'ID': test['ID']})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = set(np.argmax(predict,axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vectorization Approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n# TF-IDF feature matrix\ntrain1 = tfidf_vectorizer.fit_transform(train['combined'])\ntest1 = tfidf_vectorizer.fit_transform(test['combined'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['combined'].shape\ntest['combined'].shape\n\nnp.hstack([train['combined'].values,test['combined'].values]).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n# create the transform\nvectorizer = CountVectorizer(stop_words=\"english\", analyzer='word', \n                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n# tokenize and build vocab\nvectorizer.fit(np.hstack([train['combined'].values,test['combined'].values]))\ntrain2 = vectorizer.transform(train['combined'])\ntest2 = vectorizer.transform(test['combined'])\nprint(train2.shape)\nprint(type(train2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csr_matrix([train['Checkouts']]).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### concat both the vectors together into single."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack, coo_matrix, csr_matrix,vstack\ntrain3 = hstack((train1, train2))\n\ntest3 = hstack((test1, test2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### lightGbm"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n# splitting data into training and validation set\nxtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train3, train['MaterialType'], random_state=42, test_size=0.2)\n\n# lreg = LogisticRegression()\nlreg = lgb.LGBMRegressor()\nlreg.fit(xtrain_bow, ytrain) # training the model\n\nprediction = lreg.predict(xvalid_bow) # predicting on the validation set\n# prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\nprediction_int = prediction.astype(np.int)\n\nf1_score(yvalid, prediction_int,average='micro') # calculating f1 score\n\ntest_pred = lreg.predict(test3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n\n# splitting data into training and validation set\nxtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train3, train['MaterialType'], random_state=42, test_size=0.2)\n\nlreg = LogisticRegression()\n# lreg = lgb.LGBMRegressor()\nlreg.fit(xtrain_bow, ytrain) # training the model\n\nprediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\nprediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\nprediction_int = prediction_int.astype(np.int)\n\nf1_score(yvalid, prediction_int,average='micro') # calculating f1 score\n\ntest_pred = lreg.predict_proba(test3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB(alpha=0.8)\nnb.fit(xtrain_bow, ytrain)\n\npreds = nb.predict(xvalid_bow)\nf1_score(yvalid, preds,average='micro') # calculating f1 score\n\ntest_pred = nb.predict(test3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subfin = pd.DataFrame({'ID': test['ID'].values, 'MaterialType': test_pred})\nsubfin=subfin.reindex(columns=[\"ID\",\"MaterialType\"])\nsubfin.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### convert again to the submission format"},{"metadata":{"trusted":true},"cell_type":"code","source":"subfin['MaterialType'].replace(0,\"BOOK\",inplace=True)\nsubfin['MaterialType'].replace(1,\"CR\",inplace=True)\nsubfin['MaterialType'].replace(2,\"MIXED\",inplace=True)\nsubfin['MaterialType'].replace(3,\"MUSIC\",inplace=True)\nsubfin['MaterialType'].replace(4,\"SOUNDCASS\",inplace=True)\nsubfin['MaterialType'].replace(5,\"SOUNDDISC\",inplace=True)\nsubfin['MaterialType'].replace(6,\"VIDEOCASS\",inplace=True)\nsubfin['MaterialType'].replace(7,\"VIDEODISC\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subfin.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"submission.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe\ncreate_download_link(subfin)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}